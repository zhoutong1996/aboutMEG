<!DOCTYPE html>
<html><head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		<title>Zotero 报告</title>
		<link rel="stylesheet" type="text/css" href="data:text/css;base64,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">
		<link rel="stylesheet" type="text/css" media="screen,projection" href="data:text/css;base64,LyogR2VuZXJpYyBzdHlsZXMgKi8KYm9keSB7Cglmb250OiA2Mi41JSBHZW9yZ2lhLCBUaW1lcywgc2VyaWY7Cgl3aWR0aDogNzgwcHg7CgltYXJnaW46IDAgYXV0bzsKfQoKaDIgewoJZm9udC1zaXplOiAxLjVlbTsKCWxpbmUtaGVpZ2h0OiAxLjVlbTsKCWZvbnQtZmFtaWx5OiBHZW9yZ2lhLCBUaW1lcywgc2VyaWY7Cn0KCnAgewoJbGluZS1oZWlnaHQ6IDEuNWVtOwp9CgphOmxpbmssIGE6dmlzaXRlZCB7Cgljb2xvcjogIzkwMDsKfQoKYTpob3ZlciwgYTphY3RpdmUgewoJY29sb3I6ICM3Nzc7Cn0KCgp1bC5yZXBvcnQgewoJZm9udC1zaXplOiAxLjRlbTsKCXdpZHRoOiA2ODBweDsKCW1hcmdpbjogMCBhdXRvOwoJcGFkZGluZzogMjBweCAyMHB4Owp9CgovKiBNZXRhZGF0YSB0YWJsZSAqLwp0YWJsZSB7Cglib3JkZXI6IDFweCAjY2NjIHNvbGlkOwoJb3ZlcmZsb3c6IGF1dG87Cgl3aWR0aDogMTAwJTsKCW1hcmdpbjogLjFlbSBhdXRvIC43NWVtOwoJcGFkZGluZzogMC41ZW07Cn0K">
		<link rel="stylesheet" type="text/css" media="print" href="data:text/css;base64,Ym9keSB7Cglmb250OiAxMnB0ICJUaW1lcyBOZXcgUm9tYW4iLCBUaW1lcywgR2VvcmdpYSwgc2VyaWY7CgltYXJnaW46IDA7Cgl3aWR0aDogYXV0bzsKCWNvbG9yOiBibGFjazsKfQoKLyogUGFnZSBCcmVha3MgKHBhZ2UtYnJlYWstaW5zaWRlIG9ubHkgcmVjb2duaXplZCBieSBPcGVyYSkgKi8KaDEsIGgyLCBoMywgaDQsIGg1LCBoNiB7CglwYWdlLWJyZWFrLWFmdGVyOiBhdm9pZDsKCXBhZ2UtYnJlYWstaW5zaWRlOiBhdm9pZDsKfQoKdWwsIG9sLCBkbCB7CglwYWdlLWJyZWFrLWluc2lkZTogYXZvaWQ7Cgljb2xvci1hZGp1c3Q6IGV4YWN0Owp9CgpoMiB7Cglmb250LXNpemU6IDEuM2VtOwoJbGluZS1oZWlnaHQ6IDEuM2VtOwp9CgphIHsKCWNvbG9yOiAjMDAwOwoJdGV4dC1kZWNvcmF0aW9uOiBub25lOwp9Cg==">
	</head>
	<body>
		<ul class="report combineChildItems">
			<li id="item_TDDRZJYE" class="item conferencePaper">
			<h2>Data augmentation methods for machine-learning-based classification of bio-signals</h2>
				<table>
					<tbody><tr>
						<th>类型</th>
						<td>会议论文</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>A. Sakai</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Y. Minoda</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>K. Morikawa</td>
					</tr>
					<tr>
					<th>摘要</th>
						<td>Data augmentation methods for bio-signal classification are 
proposed. These methods improve recognition performance of human mental 
states showing intrinsic motivation from brain wave. Conventionally, 
data augmentation is used to image recognition research. Scaling, 
rotation, and distortion are applied to the original images to increase 
examples for machine learning. However, these augmentation methods are 
not effective for use with biological signals, as they involve spatial 
manipulation designed to represent the fluctuations of natural images. 
In the present study, we proposed four novel methods for data 
augmentation of biological signals. These methods are designed to 
represent variations inherent to bio-signals, especially for 
event-related signals. Electroencephalogram (EEG) data from participants
 engaged in an intrinsic motivation task were utilized to evaluate the 
feasibility of the proposed data augmentation methods. Our findings 
demonstrated that the proposed methods are particularly effective for 
improving prediction accuracy in small datasets.</td>
					</tr>
					<tr>
					<th>日期</th>
						<td>August 2017</td>
					</tr>
					<tr>
					<th>馆藏目录</th>
						<td>IEEE Xplore</td>
					</tr>
					<tr>
					<th>页码</th>
						<td>1-4</td>
					</tr>
					<tr>
					<th>投递标题</th>
						<td>2017 10th Biomedical Engineering International Conference (BMEiCON)</td>
					</tr>
					<tr>
					<th>学术会议名称</th>
						<td>2017 10th Biomedical Engineering International Conference (BMEiCON)</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1109/BMEiCON.2017.8229109">10.1109/BMEiCON.2017.8229109</a></td>
					</tr>
					<tr>
					<th>添加日期</th>
						<td>2020/10/31 下午1:08:42</td>
					</tr>
					<tr>
					<th>修改日期</th>
						<td>2020/10/31 下午1:08:42</td>
					</tr>
				</tbody></table>
				<h3 class="tags">标签：</h3>
				<ul class="tags">
					<li>brain</li>
					<li>electroencephalography</li>
					<li>Electroencephalography</li>
					<li>natural images</li>
					<li>EEG</li>
					<li>feature extraction</li>
					<li>image classification</li>
					<li>Machine learning</li>
					<li>medical image processing</li>
					<li>Training data</li>
					<li>learning (artificial intelligence)</li>
					<li>Deep Learning</li>
					<li>bio-signal classification</li>
					<li>Bio-signals</li>
					<li>biological signals</li>
					<li>Biology</li>
					<li>Brain wave</li>
					<li>Data Augmentation</li>
					<li>data augmentation methods</li>
					<li>electroencephalogram data</li>
					<li>event-related signals</li>
					<li>Image recognition</li>
					<li>image recognition research</li>
					<li>intrinsic motivation task</li>
					<li>machine-learning</li>
					<li>Presses</li>
					<li>recognition performance</li>
					<li>spatial manipulation</li>
				</ul>
				<h3 class="attachments">附件</h3>
				<ul class="attachments">
					<li id="item_Z5C4W7VY">IEEE Xplore Abstract Record					</li>
					<li id="item_RCVTQ8QZ">Sakai et al_2017_Data augmentation methods for machine-learning-based classification of.pdf					</li>
				</ul>
			</li>


			<li id="item_AIIJ7Q6D" class="item conferencePaper">
			<h2>Deep EEG super-resolution: Upsampling EEG spatial resolution with Generative Adversarial Networks</h2>
				<table>
					<tbody><tr>
						<th>类型</th>
						<td>会议论文</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>I. A. Corley</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Y. Huang</td>
					</tr>
					<tr>
					<th>摘要</th>
						<td>Electroencephalography (EEG) activity contains a wealth of 
information about what is happening within the human brain. Recording 
more of this data has the potential to unlock endless future 
applications. However, the cost of EEG hardware is increasingly 
expensive based upon the number of EEG channels being recorded 
simultaneously. We combat this problem in this paper by proposing a 
novel deep EEG super-resolution (SR) approach based on Generative 
Adversarial Networks (GANs). This approach can produce high spatial 
resolution EEG data from low resolution samples, by generating 
channel-wise upsampled data to effectively interpolate numerous missing 
channels, thus reducing the need for expensive EEG equipment. We tested 
the performance using an EEG dataset from a mental imagery task. Our 
proposed GAN model provided 104 fold and 102 fold reduction in 
mean-squared error (MSE) and mean-absolute error (MAE), respectively, 
over the baseline bicubic interpolation method. We further validate our 
method by training a classifier on the original classification task, 
which displayed minimal loss in accuracy while using the super-resolved 
data. The proposed SR EEG by GAN is a promising approach to improve the 
spatial resolution of low density EEG headset.</td>
					</tr>
					<tr>
					<th>日期</th>
						<td>March 2018</td>
					</tr>
					<tr>
					<th>短标题</th>
						<td>Deep EEG super-resolution</td>
					</tr>
					<tr>
					<th>馆藏目录</th>
						<td>IEEE Xplore</td>
					</tr>
					<tr>
					<th>页码</th>
						<td>100-103</td>
					</tr>
					<tr>
					<th>投递标题</th>
						<td>2018 IEEE EMBS International Conference on Biomedical Health Informatics (BHI)</td>
					</tr>
					<tr>
					<th>学术会议名称</th>
						<td>2018 IEEE EMBS International Conference on Biomedical Health Informatics (BHI)</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1109/BHI.2018.8333379">10.1109/BHI.2018.8333379</a></td>
					</tr>
					<tr>
					<th>添加日期</th>
						<td>2020/10/31 下午1:22:16</td>
					</tr>
					<tr>
					<th>修改日期</th>
						<td>2020/10/31 下午1:22:16</td>
					</tr>
				</tbody></table>
				<h3 class="tags">标签：</h3>
				<ul class="tags">
					<li>brain</li>
					<li>human brain</li>
					<li>electroencephalography</li>
					<li>Electroencephalography</li>
					<li>Brain modeling</li>
					<li>Convolution</li>
					<li>Training</li>
					<li>medical image processing</li>
					<li>Gallium nitride</li>
					<li>Generators</li>
					<li>GAN model</li>
					<li>generative adversarial networks</li>
					<li>image resolution</li>
					<li>Spatial resolution</li>
					<li>baseline bicubic interpolation method</li>
					<li>channel-wise upsampled data</li>
					<li>deep EEG super-resolution approach</li>
					<li>EEG equipment</li>
					<li>EEG spatial resolution</li>
					<li>electroencephalography activity</li>
					<li>interpolation</li>
					<li>mean square error methods</li>
					<li>mean-absolute error</li>
					<li>mean-squared error</li>
					<li>mental imagery task</li>
				</ul>
				<h3 class="attachments">附件</h3>
				<ul class="attachments">
					<li id="item_L3K72FFK">Corley_Huang_2018_Deep EEG super-resolution.pdf					</li>
					<li id="item_RP5CX6KX">IEEE Xplore Abstract Record					</li>
				</ul>
			</li>


			<li id="item_FRHTMS4K" class="item conferencePaper">
			<h2>Generating target/non-target images of an RSVP experiment from brain signals in by conditional generative adversarial network</h2>
				<table>
					<tbody><tr>
						<th>类型</th>
						<td>会议论文</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Y. Lee</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Y. Huang</td>
					</tr>
					<tr>
					<th>摘要</th>
						<td>Understanding human brain activities and their associations 
with sensory stimuli is an important area of brain research. We present 
in this paper e reconstruction of target and nontarget images from 
Electroencephalography (EEG) signals collected in a Rapid serial visual 
presentation (RSVP) experiment. We proposed a novel model based on 
conditional Generative Adversarial Networks (cGAN), which includes a 
generator to generate target/nontarget images from input EEG epochs and 
discriminator to discriminate true images from the generated images. We 
showed the performance of image generation of the proposed cGAN model 
based EEG or EEG plus noise as input. We further demonstrated how we 
could use the trained model to examine the associations between 
target/nontarget images and their induced EEG patterns.</td>
					</tr>
					<tr>
					<th>日期</th>
						<td>March 2018</td>
					</tr>
					<tr>
					<th>馆藏目录</th>
						<td>IEEE Xplore</td>
					</tr>
					<tr>
					<th>页码</th>
						<td>182-185</td>
					</tr>
					<tr>
					<th>投递标题</th>
						<td>2018 IEEE EMBS International Conference on Biomedical Health Informatics (BHI)</td>
					</tr>
					<tr>
					<th>学术会议名称</th>
						<td>2018 IEEE EMBS International Conference on Biomedical Health Informatics (BHI)</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1109/BHI.2018.8333399">10.1109/BHI.2018.8333399</a></td>
					</tr>
					<tr>
					<th>添加日期</th>
						<td>2020/10/31 下午1:25:32</td>
					</tr>
					<tr>
					<th>修改日期</th>
						<td>2020/10/31 下午1:25:32</td>
					</tr>
				</tbody></table>
				<h3 class="tags">标签：</h3>
				<ul class="tags">
					<li>brain</li>
					<li>electroencephalography</li>
					<li>Electroencephalography</li>
					<li>EEG</li>
					<li>Brain modeling</li>
					<li>medical signal processing</li>
					<li>neurophysiology</li>
					<li>Visualization</li>
					<li>EEG signals</li>
					<li>Gallium nitride</li>
					<li>Generators</li>
					<li>electroencephalography signals</li>
					<li>RSVP experiment</li>
					<li>image generation</li>
					<li>Image reconstruction</li>
					<li>brain signals</li>
					<li>brain research</li>
					<li>Conditional Generatie Networks</li>
					<li>conditional generative adversarial networks</li>
					<li>EEG epochs</li>
					<li>generated images</li>
					<li>human brain activities</li>
					<li>induced EEG patterns</li>
					<li>Rapid serial visual presentation (RSVP)</li>
					<li>rapid serial visual presentation experiment</li>
					<li>sensory stimuli</li>
					<li>target/nontarget images</li>
				</ul>
				<h3 class="attachments">附件</h3>
				<ul class="attachments">
					<li id="item_EPCILWBM">IEEE Xplore Abstract Record					</li>
					<li id="item_8V3QXHFV">Lee_Huang_2018_Generating target-non-target images of an RSVP experiment from brain signals in.pdf					</li>
				</ul>
			</li>


			<li id="item_ZJ8BJ6IP" class="item journalArticle">
			<h2>Improving brain computer interface performance by data 
augmentation with conditional Deep Convolutional Generative Adversarial 
Networks</h2>
				<table>
					<tbody><tr>
						<th>类型</th>
						<td>期刊文章</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Qiqi Zhang</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Ying Liu</td>
					</tr>
					<tr>
					<th>摘要</th>
						<td>One of the big restrictions in brain computer interface field 
is the very limited training samples, it is difficult to build a 
reliable and usable system with such limited data. Inspired by 
generative adversarial networks, we propose a conditional Deep 
Convolutional Generative Adversarial (cDCGAN) Networks method to 
generate more artificial EEG signal automatically for data augmentation 
to improve the performance of convolutional neural networks in brain 
computer interface field and overcome the small training dataset 
problems. We evaluate the proposed cDCGAN method on BCI competition 
dataset of motor imagery. The results show that the generated artificial
 EEG data from Gaussian noise can learn the features from raw EEG data 
and has no less than the classification accuracy of raw EEG data in the 
testing dataset. Also by using generated artificial data can effectively
 improve classification accuracy at the same model with limited training
 data.</td>
					</tr>
					<tr>
					<th>日期</th>
						<td>2018-12-27</td>
					</tr>
					<tr>
					<th>馆藏目录</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/1806.07108">http://arxiv.org/abs/1806.07108</a></td>
					</tr>
					<tr>
					<th>访问时间</th>
						<td>2020/10/20 下午8:22:26</td>
					</tr>
					<tr>
					<th>其它</th>
						<td>arXiv: 1806.07108</td>
					</tr>
					<tr>
					<th>期刊</th>
						<td>arXiv:1806.07108 [cs, q-bio, stat]</td>
					</tr>
					<tr>
					<th>添加日期</th>
						<td>2020/10/20 下午8:22:26</td>
					</tr>
					<tr>
					<th>修改日期</th>
						<td>2020/10/20 下午8:22:28</td>
					</tr>
				</tbody></table>
				<h3 class="tags">标签：</h3>
				<ul class="tags">
					<li>Computer Science - Machine Learning</li>
					<li>Statistics - Machine Learning</li>
					<li>Quantitative Biology - Neurons and Cognition</li>
					<li>Computer Science - Human-Computer Interaction</li>
					<li>Important</li>
					<li>Reference algorithm</li>
				</ul>
				<h3 class="notes">笔记：</h3>
				<ul class="notes">
					<li id="item_CFZ8FTK5">
<p class="plaintext">Comment: 4 pages, 5 figures</p>
					</li>
				</ul>
				<h3 class="attachments">附件</h3>
				<ul class="attachments">
					<li id="item_MD3ZB38I">arXiv.org Snapshot					</li>
					<li id="item_BGDNEMF9">Zhang_Liu_2018_Improving brain computer interface performance by data augmentation with.pdf					</li>
				</ul>
			</li>

		</ul>
	
</body></html>